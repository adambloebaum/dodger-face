{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import dlib\n",
    "import json\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import create_engine\n",
    "from face_encodings import Base, Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database connection\n",
    "engine = create_engine('sqlite:///face_encodings.db')\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_face_detector = dlib.cnn_face_detection_model_v1(r'C:\\Users\\adam.bloebaum\\.vscode\\driveline\\cv\\mmod_human_face_detector.dat')\n",
    "face_rec_model = dlib.face_recognition_model_v1(r'C:\\Users\\adam.bloebaum\\.vscode\\driveline\\cv\\dlib_face_recognition_resnet_model_v1.dat')\n",
    "shape_predictor = dlib.shape_predictor(r\"C:\\Users\\adam.bloebaum\\.vscode\\driveline\\cv\\shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "video_directory = r\"C:\\Users\\adam.bloebaum\\Desktop\\face_scans\"\n",
    "video_path = r\"C:\\Users\\adam.bloebaum\\Desktop\\face_scans\\IMG_1970.MOV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract every Nth frame from the video\n",
    "def extract_frames(video_path, frame_skip=30):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    frame_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_count % frame_skip == 0:\n",
    "            frames.append(frame)\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect faces\n",
    "def detect_faces(frames, cnn_face_detector):\n",
    "    detected_faces = []\n",
    "\n",
    "    for frame in frames:\n",
    "        # convert the OpenCV BGR image to RGB\n",
    "        rgb_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        faces = cnn_face_detector(rgb_image, 1)\n",
    "\n",
    "        for face in faces:\n",
    "            x, y, w, h = face.rect.left(), face.rect.top(), face.rect.width(), face.rect.height()\n",
    "            detected_faces.append(rgb_image[y:y+h, x:x+w])\n",
    "\n",
    "    return detected_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply distortions to faces for robustness\n",
    "def distort_faces(faces, rotate=True, add_noise=True, flip=True):\n",
    "    distorted_faces = []\n",
    "\n",
    "    for face in faces:\n",
    "        if rotate:\n",
    "            # rotate the face by a random angle between -30 and 30 degrees\n",
    "            (h, w) = face.shape[:2]\n",
    "            center = (w // 2, h // 2)\n",
    "            angle = np.random.uniform(-30, 30)\n",
    "            M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "            face = cv2.warpAffine(face, M, (w, h))\n",
    "\n",
    "        if add_noise:\n",
    "            # add random noise to the face\n",
    "            noise = np.random.normal(0, 15, face.shape).astype(np.uint8)\n",
    "            face = cv2.add(face, noise)\n",
    "\n",
    "        if flip:\n",
    "            # horizontally flip the face\n",
    "            face = cv2.flip(face, 1)\n",
    "\n",
    "        distorted_faces.append(face)\n",
    "\n",
    "    return distorted_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face_encodings(face):\n",
    "    # convert the OpenCV BGR image to RGB\n",
    "    rgb_image = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # initialize the Dlib face detector and shape predictor\n",
    "    face_detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "    # detect faces\n",
    "    detected_faces = face_detector(rgb_image, 1)\n",
    "    if len(detected_faces) == 0:\n",
    "        # no faces found in the image, return None or an appropriate value\n",
    "        return None\n",
    "\n",
    "    # get the landmarks/parts for the face\n",
    "    shape = shape_predictor(rgb_image, detected_faces[0])\n",
    "\n",
    "    # align and crop the face using the landmarks\n",
    "    face_chip = dlib.get_face_chip(rgb_image, shape)\n",
    "\n",
    "    # compute the face descriptor\n",
    "    face_descriptor = face_rec_model.compute_face_descriptor(face_chip)\n",
    "\n",
    "    # convert the descriptor to a numpy array, then to a list, and serialize to JSON\n",
    "    encoding = np.array(face_descriptor).tolist()\n",
    "    return json.dumps(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_face_encoding(session, encoding):\n",
    "    new_face = Face(encoding=encoding)\n",
    "    session.add(new_face)\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process each video\n",
    "def process_video(video_path):\n",
    "    frames = extract_frames(video_path)\n",
    "    print(len(frames), \"frames extracted.\")\n",
    "    faces = detect_faces(frames, cnn_face_detector)\n",
    "    print(len(faces), \"faces detected.\")\n",
    "    rotated_faces = distort_faces(faces, rotate=True, add_noise=False, flip=False)\n",
    "    noisy_faces = distort_faces(faces, rotate=False, add_noise=True, flip=False)\n",
    "    flipped_faces = distort_faces(faces, rotate=False, add_noise=False, flip=True)\n",
    "    combined_faces = faces + rotated_faces + noisy_faces + flipped_faces\n",
    "    print(len(combined_faces), \"total faces.\")\n",
    "    \n",
    "    processed_faces = 0\n",
    "    for face in combined_faces:\n",
    "        encoding = extract_face_encodings(face)\n",
    "        if encoding is not None:\n",
    "            store_face_encoding(session, encoding)\n",
    "            processed_faces += 1\n",
    "    print(\"Successfully processed\", processed_faces, \"face encodings to the database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 frames extracted.\n",
      "5 faces detected.\n",
      "5 faces rotated.\n",
      "5 faces noised.\n",
      "5 faces flipped.\n",
      "20 total faces.\n",
      "Successfully processed 10 face encodings to the database.\n"
     ]
    }
   ],
   "source": [
    "for video_file in glob.glob(os.path.join(video_directory, '*.mov')):\n",
    "    process_video(video_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the database for binary data\n",
    "faces = session.query(Face.image_data).all()\n",
    "\n",
    "# display the images\n",
    "for face in faces:\n",
    "    # convert binary data to NumPy array and then to image\n",
    "    nparr = np.frombuffer(face.image_data, np.uint8)\n",
    "    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "\n",
    "    # display the image using OpenCV\n",
    "    cv2.imshow('Face', img)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all records in the 'Face' table\n",
    "session.query(Face).delete()\n",
    "\n",
    "# commit the changes\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "driveline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
